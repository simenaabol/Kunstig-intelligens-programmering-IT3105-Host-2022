"""
On-policy RL -> policyen som brukes for s√∏k gjennom states gjennom hver episode (behavior policy) er ogs√• policyen som systemet 
                bruker for √• l√¶re (target policy)

To insure a balance between exploration and exploitation, the actor should use its policy in an -greedy
manner (see actor-critic.pdf), where  is either a constant, user-supplied parameter, or a dynamic variable
that changes (i.e. decreases) from earlier to later episodes. By setting  = 0 at run‚Äôs end, the behavior policy
essentially becomes the target policy. By displaying one game played with this policy, the user sees the best
moves that the actor has found for the states of an episode.

The actor‚Äôs policy should be represented as a table (or Python dictionary) that maps state-action pairs (s,a)
to values that indicate the desirability of performing action a when in state s. For any state s, it is wise to
normalize the values across all legal actions from s, thus yielding a probability distribution over the possible
actions to take from s. The  greedy policy would then choose the action with the highest probability, or a
random value

parametere:

learning_rate
gamma
epsilon
goal_epsilon
eligibility_decay

"""

""" class Actor():
    def __init__(self, learning_rate, gamma, epsilon, g_epsilon, eligibility_decay): """


